---
title: "Analysis of Bikeshare Data with chatGPT"
output:
  html_document:
    df_print: paged
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(message = F,
                      error = F, warning = F)
```

```{r echo = TRUE, results = 'hide'}
library(tidyverse)
library(car)
library(boot)
library(here)
library(knitr)
library(glmnet)
data <- read_csv(here("inputs\\day.csv"))
```

# Data Exploration

We can do some preliminary analysis of the data to see if particular models may
be more appropriate then others.

```{r echo = FALSE}
head(data)
```

We have some categorical variables in weathersit, which has 4 factors based on worsening weather conditions from clear, to mist, to light snow/rain, to heavy
snow/rain. And we have a season variable indicating the 4 different seasons.

```{r}
data %>%
  ggplot(aes(x = cnt)) + geom_histogram(fill = "green", color = "black")
```

The response in this case is a count of bicycles rented for the day. Count data
is most often represented by a poisson or negative/positive binomial distribution. The difference between these 2 distributions is that poisson regression is used
for count where the mean is roughly equal to the standard deviation. We check
to see whether this holds for the bikeshare data.

```{r}
data$cnt %>% sd()
data$cnt %>% mean()
```

The mean and standard deviation in the sample suggest rather larger overdispersion. Which may indicate that a quasi-poison or negative binomial
distribution will be better performing then a poisson distribution on this data.

```{r}
data %>%
  ggplot(aes(sample = cnt))+stat_qq(distribution = stats::qpois, dparams = list(lambda = mean(data$cnt)))+ stat_qq_line(distribution = stats::qpois, dparams = list(lambda = mean(data$cnt))) + 
  ylab("Data Quantiles") +
  xlab("Poisson Quantiles") +
  theme(axis.title.y = element_text(size = rel(1.5), angle = 90)) +
  theme(axis.title.x = element_text(size = rel(1.5), angle = 00))
```

The distribution and parameters estimates seem roughly poisson so we will
proceed with building a poisson regression model.

Now lets look at how some of our predictors relate to the number of bikes rented.


```{r}
data %>%
  group_by(mnth) %>%
  summarize(counts = mean(cnt)) %>%
  ungroup() %>%
  ggplot(aes(x = mnth, y = counts))+geom_line() +
  xlab("Month") +
  ylab("Average Bikes Rented") +
  scale_x_continuous(breaks = 1:12, labels = month.name) +
  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust=1))
```

As we may expect bike rentals peak in the warmer summer months and decline
during colder winter months before picking back up again as the seasons change.
It will be important we capture the cyclical nature of this strong predictor
for the bike rentals in our model.

Similarly we visualize the days of the week
```{r}
data %>%
  group_by(weekday) %>%
  summarize(counts = mean(cnt)) %>%
  ungroup() %>%
  ggplot(aes(x = weekday, y = counts))+geom_line() +
  xlab("Day") +
  ylab("Average Bikes Rented") +
  scale_x_continuous(breaks = c(0,1,2,3,4,5,6), labels = c("Mon", "Tues",
                                                         "Wed", "Thur", "Fri", "Sat", "Sun")) +
  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust=1))
```

Its interesting that the later days of the week appear to be more popular for renting a bike. We can get a precise look at that count:

```{r}
data %>%
  group_by(weekday) %>%
  summarize(n = sum(cnt))
```


```{r}
data %>%
  dplyr::select(cnt, workingday) %>%
  group_by(workingday) %>%
  summarize(counts = mean(cnt)) %>%
  ungroup() %>%
  ggplot(aes(x = workingday, y = counts, fill = workingday))+geom_bar(stat = "identity", color = "black") +
  theme(legend.position = "none") +
  ylab("Average Bike Rentals") +
  xlab("Working Day") +
  scale_y_continuous(limits = c(0,5000)) +
  scale_x_continuous(breaks = c(0,1), labels = c("No", "Yes"))
```

We see little difference in the average rentals on working days as non working
days. This suggests that this predictor will not be a strong indicator in our model for predicting the number of bikes rented.

We now visualize some of the continuous variables present in the data and their
relationship with rental counts.

```{r}
ggplot_helper <- function(data, x){
  ggplot(data = data, aes(x = {{x}}, y = cnt)) + geom_point()+
  ylab("Rentals")
}
ggplot_helper(data, x = hum)
ggplot_helper(data, temp)
ggplot_helper(data, windspeed)
```



# Model Building

First we can try a basic 80/20 validation and test split and fit the full poisson
regression model. We make slight adjustments to the features to ensure
dates are encoded as datetime objects and remove variables like instant which
just refer to the observation number, we also remove registered and casual
since these variables are just leakage of the counts into the predictors.
Then compute the mean square errors on the test data.

```{r}
# Convert date to Date type
data$date <- as.Date(data$dteday)

# Remove unnecessary columns
data <- data %>% dplyr::select(-instant, -dteday, -casual, -registered)
```

We convert dteday to a datetime column and remove casual and registered as those are simply leakage variables that directly correlate with the rental count.

```{r}

# Split the data into training and testing sets
set.seed(1)
train_indices <- sample(1:nrow(data), size = 0.8 * nrow(data))
train_data <- data[train_indices,]
test_data <- data[-train_indices,]

# Fit Poisson regression model
model <- glm(cnt ~ ., data = train_data, family = "poisson")
summary(model)

lm.fit <- lm(cnt ~ ., data = train_data)

# Make predictions
test_data$pred_count <- predict(model, newdata = test_data, type = "response")
test_data$pred_count_lm <- predict(lm.fit, newdata = test_data, type = "response")

# Calculate root mean squared error (RMSE)
rmse <- sqrt(mean((test_data$cnt - test_data$pred_count)^2))
cat("Root Mean Squared Error (RMSE):", rmse, "\n")

rmse_lm <- sqrt(mean((test_data$cnt - test_data$pred_count_lm)^2))
cat("Root Mean Squared Error of LM (RMSE):", rmse_lm, "\n")

test_data <- test_data %>% dplyr::select(-pred_count, -pred_count_lm)
```

Currently we see the linear regression model actually outperforming the poisson
regression, which should suggest a problem as our count data naturally is better
suited to the poisson distriubtion. We will do some additional feature engineering. We will encode the datetime as a cyclical variable using sine and cosine functions. This will establish that for instance January is closer to December then it is to April.


```{r}
#cyclical encoding of mnth, weekday
train_data$mnth_sin <- sin(2 * pi * train_data$mnth / 12)
train_data$mnth_cos <- cos(2 * pi * train_data$mnth / 12)

train_data$weekday_sin <- sin(2 * pi * train_data$weekday / 7)
train_data$weekday_cos <- cos(2 * pi * train_data$weekday / 7)

test_data$mnth_sin <- sin(2 * pi * test_data$mnth / 12)
test_data$mnth_cos <- cos(2 * pi * test_data$mnth / 12)

test_data$weekday_sin <- sin(2 * pi * test_data$weekday / 7)
test_data$weekday_cos <- cos(2 * pi * test_data$weekday / 7)

data$mnth_sin <- sin(2 * pi * data$mnth / 12)
data$mnth_cos <- cos(2 * pi * data$mnth / 12)

data$weekday_sin <- sin(2 * pi * data$weekday / 7)
data$weekday_cos <- cos(2 * pi * data$weekday / 7)
```

We also have some interactions among these predictors, for example the combination of cold temperatures and rain/snow is probably less desirable then just a cold or just a rainy day for riding a bike. We'll also add non linear terms for the continuous variables such as temperature, humidity, and windspeed. 

```{r}
# Fit Poisson regression model
model <- glm(cnt ~ . + weathersit*temp, data = train_data, family = poisson())
summary(model)

lm.fit <- lm(cnt ~ . + weathersit*temp, data = train_data)

# Make predictions
test_data$pred_count <- predict(model, newdata = test_data, type = "response")
test_data$pred_count_lm <- predict(lm.fit, newdata = test_data, type = "response")

# Calculate root mean squared error (RMSE)
rmse <- sqrt(mean((test_data$cnt - test_data$pred_count)^2))
cat("Root Mean Squared Error (RMSE):", rmse, "\n")

rmse_lm <- sqrt(mean((test_data$cnt - test_data$pred_count_lm)^2))
cat("Root Mean Squared Error of LM (RMSE):", rmse_lm, "\n")

test_data <- test_data %>% select(-pred_count, -pred_count_lm)
```

We get a slight decrease in the square error in the test. We now will try
analyzing the residuals to see if there are some higher order relationships
that the model does not currently capture.

```{r}
# Creating a dataframe containing residuals and fitted values
df <- broom::augment(model)

# Residuals vs. fitted values plot
ggplot(df, aes(x = .fitted, y = .resid/sqrt(.fitted))) +
  geom_point(alpha = 0.5) +
  geom_hline(yintercept = 0, linetype = "dashed") +
  labs(title = "Residuals vs. Fitted Values", x = "Fitted Values", y = "Pearson Residuals")

# Residuals vs. continuous predictors fucntion helper
resid.check <- function(data, x, y) {
  ggplot(data, aes(x = {{x}}, y = {{y}})) +
    geom_point(alpha = 0.5) +
    geom_hline(yintercept = 0, linetype = "dashed") +
    labs(title = "Residuals Plot", y = "Pearson Residuals") +
    geom_smooth()
}
resid.check(df, temp, y = .resid/sqrt(.fitted))
resid.check(df, x = hum, y = .resid/sqrt(.fitted))
resid.check(df, x = windspeed, y = .resid/sqrt(.fitted))
```

We check our continuous predictors and find a definite pattern within the temp
predictor and the hum predictor, suggesting there is a relationship not being captured by the model.

We can adjust for this by adding non linear transformations of temp and hum.

```{r}
# Fit Poisson regression model
model <- glm(cnt ~ . - temp + weathersit*temp + poly(temp,3) - hum + poly(hum,4), data = train_data, family = poisson())

lm.fit <- lm(cnt ~ . - temp + weathersit*temp + poly(temp,3) - hum + poly(hum,4), data = train_data)
# Make predictions
test_data$pred_count <- predict(model, newdata = test_data, type = "response")
test_data$pred_count_lm <- predict(lm.fit, newdata = test_data, type = "response")

# Calculate root mean squared error (RMSE)
rmse <- sqrt(mean((test_data$cnt - test_data$pred_count)^2))
cat("Root Mean Squared Error (RMSE):", rmse, "\n")

rmse_lm <- sqrt(mean((test_data$cnt - test_data$pred_count_lm)^2))
cat("Root Mean Squared Error (RMSE):", rmse_lm, "\n")

test_data <- test_data %>% dplyr::select(-pred_count)
```

We see significant reductions in RMSE for both models however the particularly
in the poisson model which now performs better then the linear regression model
as we had expected. We can check the residual plots again with this new model.

```{r}
#Checking residual values after adjusting model for nonlinear values

df <- broom::augment(model)

resid.check(df, x = temp, y = .resid/sqrt(.fitted))
resid.check(df, x = hum, y = .resid/sqrt(.fitted))
```

Now we get a much more randomly scattered plot of the residuals after the
transformations. We can add these additional predictors directly into the dataset.

```{r}
#Creating polynomial terms for temp
train_data <- train_data %>% dplyr::mutate(temp_2 = temp^2, temp_3 = temp^3)
test_data <- test_data %>% dplyr::mutate(temp_2 = temp^2, temp_3 = temp^3)

#Creating polynomial terms for hum
train_data <- train_data %>% dplyr::mutate(hum_2 = hum^2, hum_3 = hum^3, hum_4 = hum^4)
test_data <- test_data %>% dplyr::mutate(hum_2 = hum^2, hum_3 = hum^3, hum_4 = hum^4)
```

We now refit the model.

```{r}
# Fit Poisson regression model
model <- glm(cnt ~ . - temp + weathersit*temp, data = train_data, family = poisson())
summary(model)

lm.fit <- lm(cnt ~ . - temp + weathersit*temp, data = train_data)

# Make predictions
test_data$pred_count <- predict(model, newdata = test_data, type = "response")
test_data$pred_count_lm <- predict(lm.fit, newdata = test_data, type = "response")

# Calculate root mean squared error (RMSE)
rmse <- sqrt(mean((test_data$cnt - test_data$pred_count)^2))
cat("Root Mean Squared Error (RMSE):", rmse, "\n")

rmse_lm <- sqrt(mean((test_data$cnt - test_data$pred_count_lm)^2))
cat("Root Mean Squared Error of LM (RMSE):", rmse_lm, "\n")

test_data <- test_data %>% dplyr::select(-pred_count, -pred_count_lm)
```


We may try lasso regression as a method for variable selection. If dimension
reduction would improve this model we should see this method reduce RMSE. However
due to the overall size of this model still being relatively small relative to
the sample size it's unlikely to be the case.

```{r}
# Splitting train and test sets for glmnet()
set.seed(1)
X <- model.matrix(cnt ~ . -1, data = train_data)
X.test <- model.matrix(cnt ~ . -1, data = test_data)
Y <- train_data$cnt
Y.test <- test_data$cnt
# Fit LASSO model
lasso <- glmnet(X, Y, family = "poisson" , alpha = 1)
# Cross Validating lambda values
lasso.cv <- cv.glmnet(X, Y, family = "poisson", alpha = 1)
# Finding optimal lambda
optimal <- lasso.cv$lambda.min
# Fit the optimal lambda model
lasso.mod <- glmnet(X, Y, family = "poisson", alpha = 1, lambda = optimal)

# Make predictions
lasso.pred <- predict(lasso, optimal, newx = X.test)

cat("Root Mean Squared Error (RMSE) for LOOCV:", sqrt(mean((lasso.pred-Y.test)^2)), "\n")

```

We see a rather large increase in RMSE in this case, suggesting that our original model is not overfitting, however we can check this via cross-validation.

```{r}
set.seed(1)

# Fit poisson regression model
glm.fit <- glm(cnt ~ .  + weathersit*temp, data = data,
                 family = poisson())

# Initialize a vector to store 10 fold CV errors
cv.error.10 <- rep(0,10)

# Perform 10-fold CV and store errors
for (i in 1:10) {
  cv.error.10[i] <- cv.glm(data, glm.fit, K = 10)$delta[1]
}

# Compute LOOCV
cv.err <- cv.glm(data, glm.fit)
cat("Root Mean Squared Error (RMSE) for LOOCV:", sqrt(cv.err$delta[1]), "\n")
```

We compute LOOCV in this case because the dataset is not too large. 